# ğŸš€ Kubernetes Day 5 - Scaling, Requests & Limits, and Metrics

## ğŸ“š Topics Covered

- Kubernetes Requests and Limits
- Importance of Resource Management
- Installing Metrics Server
- Demo: Requests and Limits in Action
- Scaling in Kubernetes
- Horizontal vs Vertical Scaling
- HPA (Horizontal Pod Autoscaler) vs VPA (Vertical Pod Autoscaler)
- Cluster Autoscaling vs Node Auto-Provisioning
- Simulated Load and HPA Demo

---

## âš™ï¸ Kubernetes Requests and Limits

### ğŸ§  Why Do We Need Them?
- **Requests**: Minimum guaranteed resources a pod needs.
- **Limits**: Maximum resources a pod can consume.
- Prevents resource starvation and ensures fair scheduling.

### ğŸ’» Example Manifest:

```yaml
resources:
  requests:
    memory: "64Mi"
    cpu: "250m"
  limits:
    memory: "128Mi"
    cpu: "500m"
```

---

## ğŸ“ˆ Installing Metrics Server

### ğŸ”§ Commands to Install:

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

### ğŸ§ª Verify Installation:

```bash
kubectl top nodes
kubectl top pods
```

---

## ğŸ”¬ Demo: Requests and Limits

### ğŸ› ï¸ Deploy a Pod With Limits:

```bash
kubectl run test-pod --image=nginx --requests=cpu=100m,memory=64Mi --limits=cpu=200m,memory=128Mi
```

### ğŸ§ª Check Resources:

```bash
kubectl describe pod test-pod
kubectl top pod test-pod
```

---

## ğŸ“ˆ Scaling in Kubernetes

### â• Horizontal Scaling (More Pods)
### â¬†ï¸ Vertical Scaling (More CPU/Memory per Pod)

---

## âš–ï¸ HPA vs VPA

| Feature | HPA | VPA |
|--------|-----|-----|
| Scales Pods | âœ… | âŒ |
| Adjusts Pod Size | âŒ | âœ… |
| Metrics Used | CPU/Memory/Custom | CPU/Memory |
| Use Case | Dynamic Load | Stable Workloads |

---

## ğŸ”„ Cluster Autoscaling vs Node Auto-Provisioning

- **Cluster Autoscaler**: Adjusts number of nodes based on pod demand.
- **Node Auto-Provisioning**: Automatically adds node types (machine families).

---

## ğŸš€ Demo: HPA with Load Simulation

### ğŸ“¦ Create Deployment:

```bash
kubectl create deployment cpu-stress --image=busybox -- /bin/sh -c "while true; do :; done"
```

### ğŸ“ˆ Expose as a service:

```bash
kubectl expose deployment cpu-stress --port=80 --target-port=80 --type=ClusterIP
```

### ğŸ“‰ Apply HPA:

```bash
kubectl autoscale deployment cpu-stress --cpu-percent=50 --min=1 --max=5
```

### ğŸ”¥ Simulate Load (in a separate shell):

```bash
kubectl run -i --tty load-generator --image=busybox /bin/sh
# Inside shell:
while true; do wget -q -O- http://cpu-stress; done
```

### ğŸ“Š Monitor:

```bash
kubectl get hpa
kubectl top pods
```

---

## âœ… Conclusion

I learned how to:
- Manage pod resources using requests and limits.
- Install and use the metrics server.
- Scale your workloads horizontally using HPA.
- Compare horizontal and vertical scaling strategies.
